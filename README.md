Developing a model to classify glitches in the aLIGO detectors.
================
by Micaiah Balonek

## Summary

The Laser Interferometer Gravitational-wave Observoratory, or LIGO, is a
pair of detectors, one in Washington and one in Louisiana. Their major
putpose is to detect gravitational waves, ripples in spacetime. These
detectors have to be very sensitive, so that they can detect these
changes. However, being so sensitive means that the detectors are prone
to interference from many sources, called ‘glitches’. These can prevent
detection of gravitational waves and impede searches through the data.
To mitigate glitches, the LIGO team categorise them, usually based off
their appearance in a spectrogram, so that they can model them. There
was so much data that they created a citizen science project, Gravity
Spy, where anyone could attempt to classify the glitches. They used the
data generated by all these volunteers to create a visually-trained
Machine Learning algorithm that predicts glitch types nearly perfectly.
The goal of my project was to create a similar (albeit less accurate)
model, but predicting glitch class based entirely off of the metadata of
the glitches. To do this, I started by analysing the distributions of
glitches across several pairs of variables, some of which turned out to
be completely useless, and others of which were extremely predictive.
Ultimately, I had narrowed my model down to five predictor variables:
the Interferometer the glitch was observed at, the peak frequency,
amplitude, and duration of the glitch, as well as the glitch’s
Signal-to-Noise Ratio (SNR), since these were the most clearly
correlated to glitch class. I then trained a multinomial regression
model on the (conveniently pre-seperated) training data, and used that
to predict the glitch classes. This model predicted many of the more
common classes at greater than 90% accuracy, but was worse at predicting
the ‘quieter’ (lower-SNR) glitches, as well as those at high
frequencies. This is interesting, seeing that, just by looking at a
spectrogram, one can easily distinguish between most of these classes,
excepting edge cases. Most of what I had seen in the graphs was
predicted accurately, but a few notable things weren’t, so I thought of
and implemented a few possible fixes: class splits (for example, Violin
modes come in multiple frequency bands, so, to predict them seperately
from the other high-frequency glitches, it would be useful to give each
band its own label) and outlier removal (example: There were notable
outliers in the lower-frequency glitch classes, which, if removed from
the training data, could possibly increase the accuracy of the
predictions).

From here, I applied simple combinations of class splits and outlier
removal to the initial dataframe, and then compared these new models,
finding the one that fit best. While this model predicted some glitch
classes better, many of the common classes were predicted slightly
worse: the accuracy of the high-frequency-glitches was reduced (other
than 1080 lines), and has also reduced the probabilities of accurately
predicting most common classes. Generally, the lower-frequency, quieter
glitches are the ones that the second model predicts better, while the
stronger glitches are predicted better by the first model.

Generally, both the models with and without the corrections were equally
accurate, as predicting between 75% and 80% of the glitches correctly,
just with errors in different areas of the glitch hyperspace, so I
decided to choose the more simple of the models as the representative.

Possible improvements I could make to these models in the future
include:

- Creating separate models for different parts of the distribution or
  for different interferometers; the fact that they have different
  distributions of glitches mean that this is likely to make the model
  more accurate.
- Using the time of day or year as a predictor; some glitches have been
  shown to occur more often at certain times of the day or year.
- More complicated sub-dividisions of glitch classes; I tried using this
  for Violin Mode Harmonics, but it resulted little to no improvement in
  the model. This could possibly be fixed by running other glitches
  through the division algorithm as well, resulting in a better overall
  fit. This could possibly allow for predictions of Light Modulation
  glitches, as well.
- Allowing for other functions in the regression model. my current
  regression uses the `nnet::multinom` model, which uses *linear*
  predictors to predict the data. Using more complex polynomial
  predictors would create a better approximation of the actual
  dataforms.

However, while no model like this could even come close to the accuracy
of the official spectrogram-based machine learning system, this is still
a good proof-of-concept for the prospect of predicting glitch class
using regression models with manually-adjusted data.

## Presentation

My presentation can be found [here](presentation/presentation.html).

## Data

Data from: Coughlin, S. (2018). Gravity Spy Training Set (v1.1.0) \[Data
set\]. Zenodo. <https://doi.org/10.5281/zenodo.1486046>

published under Creative Commons BY 4.0 Deed:
<https://creativecommons.org/licenses/by/4.0/>

Retrieved March 8, 2024.

I have modified the data to use it in this project, as is visible in the
file ‘extra/data-analysis.Rmd’.

## Other References

- [About Gravity
  Spy](https://www.zooniverse.org/projects/zooniverse/gravity-spy/about/research)

- [LIGO Lab website](https://www.ligo.caltech.edu/)

- [Article about the official Gravity Spy Machine Learning
  system](https://doi.org/10.1016/j.ins.2018.02.068)
